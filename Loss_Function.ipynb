{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o_mX8LxxZqA1",
        "y_5L-smtL0Ix",
        "siPOI8pZWjlj"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameshcodes/deep-learning-theory/blob/main/Loss_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\textbf{Loss Functions}$$\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "R_WUiIdfHabW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{I. Loss functions for Regression Tasks}$"
      ],
      "metadata": {
        "id": "o_mX8LxxZqA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1. Mean Squared Error (MSE) Loss}$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jJ4K2nGxQwhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Case: $\\text{Regression tasks}$\n",
        "\n",
        "### Formula:\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "- $n$ = total number of observations,\n",
        "- $y_i$ = actual value of the $i$-th observation,\n",
        "- $\\hat{y}_i$ = predicted value for the $i$-th observation.\n",
        "\n",
        "### Key Characteristics\n",
        "---\n",
        "\n",
        "- **Penalizes large errors**: Due to squaring the error, larger differences between predicted and actual values are penalized more severely. This makes MSE highly sensitive to outliers.\n",
        "- **Always positive**: Since errors are squared, MSE is always non-negative.\n",
        "- **Units**: The units of MSE are squared units of the target variable, making it harder to interpret directly in terms of the original values.\n",
        "\n",
        "### Advantages\n",
        "---\n",
        "\n",
        "- **Smooth loss function**: Easy to compute derivatives for optimization algorithms like gradient descent.\n",
        "- **Widely used**: MSE is a standard metric, often used for model evaluation and comparison.\n",
        "\n",
        "### Disadvantages\n",
        "---\n",
        "\n",
        "- **Sensitivity to outliers**: Since large errors are squared, the presence of outliers can disproportionately increase the MSE.\n",
        "- **Interpretability**: Due to squaring the errors, the result is in squared units, which may not be easy to interpret directly in the context of the original data.\n",
        "\n",
        "\n",
        "### When to Use MSE\n",
        "---\n",
        "\n",
        "- When you want to heavily penalize large errors and are concerned with both the magnitude and the distribution of errors.\n",
        "- Suitable when there are no significant outliers or outliers are acceptable.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "T56pZyG2Ho8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Implementation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "J_JUFKN8aBdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "y = torch.tensor([3.0, -0.5, 2.0, 7.0])\n",
        "y_hat = torch.tensor([2.5, 0.0, 2.0, 8.0])\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "mse = mse_loss(y_hat, y)\n",
        "\n",
        "print(f\"Mean Squared Error in pytorch implentation: {mse.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKYkDkbhHoZo",
        "outputId": "e7aa13b7-67e4-47ec-d42f-cc24c7b8f139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error in pytorch implentation: 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numpy Implementation\n",
        "---"
      ],
      "metadata": {
        "id": "qL6NtK3tLK0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_actual = np.array([3, -0.5, 2, 7])\n",
        "y_predicted = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "mse = np.mean((y_actual - y_predicted)**2)\n",
        "\n",
        "print(f\"Mean Squared Error in python implementation: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtXpi-tnLKY1",
        "outputId": "f9d7790a-7ab0-412f-a9d1-0fc80f88ed96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{2. Mean Absolute Error (MSE) Loss}$"
      ],
      "metadata": {
        "id": "y_5L-smtL0Ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Use case : Regression tasks**\n",
        "\n",
        "## Formula:\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |$$\n",
        "\n",
        "\n",
        "- $n$: The number of observations.\n",
        "- $y_i$: The actual value at instance \\(i\\).\n",
        "- $\\hat{y}_i\\$: The predicted value at instance \\(i\\).\n",
        "\n",
        "- **Units**: MAE is expressed in the same units as the target variable.\n",
        "- **Error Magnitude**: Represents the average absolute difference between predictions and actual values.\n",
        "- **No Direction Bias**: MAE does not indicate whether predictions are over or underestimations.\n",
        "\n",
        "## Advantages\n",
        "\n",
        "---\n",
        "- **Simplicity**: Easy to understand and compute.\n",
        "- **Robustness to Outliers**: Less sensitive to large errors compared to metrics like Mean Squared Error (MSE).\n",
        "- **Interpretability**: Provides a direct measure of the average error.\n",
        "\n",
        "## Disadvantages\n",
        "\n",
        "---\n",
        "- **Non-Differentiability**: The absolute value function is not differentiable at zero, which can complicate optimization algorithms like gradient descent.\n",
        "- **Equal Weight to Errors**: MAE assigns equal weight to all errors, which may not be ideal in all contexts.\n",
        "\n",
        "## When to Use MAE\n",
        "\n",
        "---\n",
        "\n",
        "MAE is a good choice in the following situations:\n",
        "\n",
        "1. **Interpretability**: If you need a metric that is easy to interpret in the same units as your target variable, MAE is ideal. It directly reflects the average error in the predicted values.\n",
        "\n",
        "2. **Robustness to Outliers**: Use MAE when you want to reduce the influence of outliers compared to other metrics like Mean Squared Error (MSE). Since MAE does not square the errors, it gives a more balanced view of the overall performance, making it suitable when extreme errors are less important.\n",
        "\n",
        "3. **Equal Weight to Errors**: If all prediction errors should be treated equally, regardless of their magnitude, MAE is appropriate. Unlike MSE, which gives more weight to larger errors, MAE considers all errors equally.\n",
        "\n",
        "4. **Non-Smooth Cost Functions**: In cases where you don’t mind non-smooth loss functions (because of the absolute value), and optimization methods can handle it, MAE can be a good fit. Some robust regression models use MAE as their loss function.\n",
        "\n",
        "\n",
        "### When Not to Use MAE\n",
        "\n",
        "---\n",
        "- **When Large Errors Matter More**: If you need a metric that penalizes large errors more heavily, Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) would be a better choice.\n",
        "- **Optimization with Gradient Descent**: If your model relies on gradient-based optimization, the non-differentiability of MAE at zero could be problematic. Smooth loss functions like MSE might be preferred.\n",
        "\n"
      ],
      "metadata": {
        "id": "5XAcpDExL8x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python implementation\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1wCzvFOLMcBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "actual = np.array([50, 60, 55, 70])\n",
        "predicted = np.array([47, 65, 53, 68])\n",
        "\n",
        "mae = np.mean(np.abs(actual - predicted))\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7adeLyArPwiA",
        "outputId": "ec057743-db4c-4ae0-912d-9775caf59633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Implementation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VBSFK9x_P5jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "actual = torch.tensor([50, 60, 55, 70], dtype=torch.float32)\n",
        "predicted = torch.tensor([47, 65, 53, 68], dtype=torch.float32)\n",
        "\n",
        "\n",
        "mae_loss = nn.L1Loss()\n",
        "\n",
        "\n",
        "mae = mae_loss(predicted, actual)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXijMHIoQBNH",
        "outputId": "89225a5b-3138-487d-b913-ed5ce99b945c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{3.Root Mean Squared Error (RMSE)}$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "siPOI8pZWjlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Use Case:** - Regression Task\n",
        "\n",
        "**Formula:**\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "**Advantages:**\n",
        "- **Interpretability:** Easier to understand as it is in the same units as the target variable.\n",
        "- **Standard Deviation Representation:** Provides insight into the spread of prediction errors.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Sensitive to Outliers:** Large residuals disproportionately affect RMSE.\n",
        "- **Not Robust to Non-Normal Errors:** May not fully capture model performance if residuals are not normally distributed.\n",
        "\n",
        "**When to Use RMSE:**\n",
        "- When you need a metric in the same units as the target variable.\n",
        "- When you want to penalize large errors more heavily and interpret results in the context of the target variable's units.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "6iqF4R5qWgkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Implementation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5HS4DMa7aJPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "predictions = torch.tensor([2.5, 0.0, 2.1, 7.8], dtype=torch.float32)\n",
        "targets = torch.tensor([3.0, -0.5, 2.0, 7.5], dtype=torch.float32)\n",
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss,self).__init__()\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        criterion = nn.MSELoss()\n",
        "        eps = 1e-9\n",
        "        loss = torch.sqrt(criterion(x, y) + eps)\n",
        "        return loss\n",
        "\n",
        "rmse_loss = RMSELoss()\n",
        "rmse = rmse_loss(predictions, targets)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) using PyTorch: {rmse.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Te5QFsWi2G",
        "outputId": "2525698f-43e0-44bc-f582-c168d6a160fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) using PyTorch: 0.3872983753681183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numpy Implementation\n",
        "---"
      ],
      "metadata": {
        "id": "G8az_fTiZoKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual calculation using NumPy\n",
        "predictions_np = np.array([2.5, 0.0, 2.1, 7.8])\n",
        "targets_np = np.array([3.0, -0.5, 2.0, 7.5])\n",
        "mse = np.mean((predictions_np - targets_np) ** 2)\n",
        "rmse_np = np.sqrt(mse)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) using NumPy: {rmse_np}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmZqslORZZFC",
        "outputId": "6c014594-7e89-4be8-b09a-065995c9939e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) using NumPy: 0.38729833462074165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{4. Mean Absolute Percentage Error (MAPE) Loss}$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VCx0pme9idua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Definition\n",
        "---\n",
        "Mean Absolute Percentage Error (MAPE) measures the accuracy of a forecasting method by calculating the average percentage difference between actual values and predicted values.\n",
        "\n",
        "### Use Case: $\\text{Regression and Forcasting}$\n",
        "\n",
        "### Formula\n",
        "$$ˇ{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\%$$\n",
        "\n",
        "where:\n",
        "- $n$ = total number of observations,\n",
        "- $y_i$ = actual value of the $i$-th observation,\n",
        "- $\\hat{y}_i$ = predicted value of the $i$-th observation.\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "---\n",
        "\n",
        "- **Percentage-based**: Expresses errors as a percentage of the actual values, making it easier to interpret relative to the scale of the data.\n",
        "- **Non-negative**: MAPE is always non-negative and gives an average percentage error across all observations.\n",
        "- **Interpretability**: Results are in percentage terms, which is intuitive and easy to understand.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "---\n",
        "\n",
        "- **Scale-independent**: Since it uses percentages, MAPE is not affected by the scale of the data, allowing for comparisons across different datasets or models.\n",
        "- **Intuitive**: Provides a clear and easily interpretable measure of forecast accuracy.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "---\n",
        "\n",
        "- **Sensitive to zero values**: When actual values $y_i$ are close to zero, MAPE can become extremely large or undefined, leading to potential issues with interpretation.\n",
        "- **Asymmetric**: MAPE does not differentiate between overestimations and underestimations equally. For instance, an overestimation and an underestimation of the same magnitude will have different impacts on MAPE.\n",
        "\n",
        "\n",
        "### When to Use MAPE\n",
        "\n",
        "---\n",
        "\n",
        "- When you need a **percentage-based measure** of forecasting accuracy and want to understand errors relative to the size of the actual values.\n",
        "- Suitable when actual values are **not close to zero** and when interpretability in percentage terms is desired.\n",
        "\n",
        "### Common Questions\n",
        "\n",
        "---\n",
        "\n",
        "- **What happens if actual values are zero or near zero?**  \n",
        "  MAPE can become very large or undefined if actual values are zero or near zero, potentially distorting the error measure.\n",
        "\n",
        "- **How does MAPE handle overestimation and underestimation?**  \n",
        "  MAPE treats overestimations and underestimations equally, as it measures the absolute percentage error.\n",
        "\n",
        "### Real-World Example\n",
        "\n",
        "---\n",
        "\n",
        "Suppose you are forecasting monthly sales and the actual sales for a month are $\\$500$, and your model predicts $\\$450$. The absolute percentage error for this prediction would be:\n",
        "\n",
        "$$\\left| \\frac{500 - 450}{500} \\right| \\times 100\\% = 10\\%$$\n",
        "\n",
        "If you have multiple such forecasts, averaging these errors gives you the MAPE for your model's performance.\n"
      ],
      "metadata": {
        "id": "YihHdi7FiwHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numpy Implementation"
      ],
      "metadata": {
        "id": "xKgSIGM5laSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Direct MAPE calculation\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Example data\n",
        "y_true = np.array([100, 200, 300, 400, 500])\n",
        "y_pred = np.array([110, 195, 290, 405, 510])\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_value = mape(y_true, y_pred)\n",
        "print(f'MAPE: {mape_value}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqZxl9hhleuU",
        "outputId": "ccc5bc3f-6b76-4a38-cf07-81936fb07172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE: 3.816666666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch - torchmetric Implementation"
      ],
      "metadata": {
        "id": "U9vfIYvBZhz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_fJKz9amdw3",
        "outputId": "e6d05bf3-c07f-448e-e2e5-6931c84b6954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/866.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m860.2/866.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torchmetrics.regression import MeanAbsolutePercentageError\n",
        "target = tensor([100, 200, 300, 400, 500])\n",
        "preds = tensor([110, 195, 290, 405, 510])\n",
        "mean_abs_percentage_error = MeanAbsolutePercentageError()\n",
        "mape = mean_abs_percentage_error(preds, target)*100\n",
        "\n",
        "print(f'MAPE: {mape}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keksub5Fmfbm",
        "outputId": "1ddc5e53-c4cb-4e9f-bf0a-0cadbc8695d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE: 3.816666841506958%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{II. Loss functions for Classfication Tasks}$"
      ],
      "metadata": {
        "id": "gru0krrgZ92k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1. Binary Cross Entropy Loss (BCE) Loss}$\n"
      ],
      "metadata": {
        "id": "8QPMp7gYaNaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Use Case:** Binary Classification tasks\n",
        "\n",
        "**Formula:**\n",
        "$$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
        "\n",
        "where:\n",
        "- $n$ = total number of observations\n",
        "- $y_i$ = actual binary label for the i-th observation\n",
        "- $\\hat{y}_i$ = predicted probability for the i-th observation (between 0 and 1)\n",
        "\n",
        "### Advantages\n",
        "\n",
        "---\n",
        "- **Directly interpretable in probability space:** The output values are probabilities, making the interpretation straightforward for classification tasks.\n",
        "- **Logarithmic penalization:** Assigns high penalty to incorrect confident predictions (i.e., very high or very low predicted probabilities that are incorrect).\n",
        "- **Sensitivity to confidence:** Penalizes confident but incorrect predictions heavily, which encourages the model to calibrate probabilities accurately.\n",
        "- **Smooth and differentiable:** This makes it suitable for optimization using gradient-based methods.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "---\n",
        "\n",
        "- **Sensitive to imbalanced datasets:** The loss can be dominated by the majority class if the dataset is imbalanced, leading to poor performance on the minority class.\n",
        "- **Overconfident predictions:** Large errors for predictions that are close to 0 or 1 can lead to unstable gradients during training.\n",
        "\n",
        "### When to Use BCE\n",
        "\n",
        "---\n",
        "- When performing binary classification and you need the model to output $probabilities$.\n",
        "- Suitable when both false positives and false negatives have different costs or significance.\n"
      ],
      "metadata": {
        "id": "ZDNoETMRcyM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Implementation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "g1mi1hpK8EOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "y_pred = torch.tensor([0.4498, 0.8845, 0.4576, 0.6450, 0.2304], requires_grad=True)\n",
        "y_true = torch.tensor([0, 1, 1, 0, 1], dtype = torch.float32)\n",
        "\n",
        "\n",
        "bce_loss = nn.BCELoss()\n",
        "bce_with_logits_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "loss_1 = bce_loss(y_true, y_pred)\n",
        "print('Binary Cross Entropy Loss:', loss_1.item())\n",
        "\n",
        "\n",
        "loss_2 = bce_with_logits_loss(y_true, y_pred)\n",
        "print('Binary Cross Entropy with Logits Loss:', loss_2.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7IArVRZaNEl",
        "outputId": "8326a0cf-03ad-4dac-c9a1-bc50230654f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross Entropy Loss: 50.44600296020508\n",
            "Binary Cross Entropy with Logits Loss: 0.7507158517837524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred, eps=1e-9):\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "    bce = -( (1/len(y_true)) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) )\n",
        "    return bce\n",
        "\n",
        "\n",
        "\n",
        "y_pred = np.array(torch.tensor([0.4498, 0.8845, 0.4576, 0.6450, 0.2304]))\n",
        "y_true = np.array([0, 1, 1, 0, 1], dtype = np.float32)\n",
        "\n",
        "loss = binary_cross_entropy(y_true, y_pred)\n",
        "print(\"Binary Cross Entropy Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqlv0wKiZ8-c",
        "outputId": "aa200eba-8282-4267-c7ad-14f339622637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross Entropy Loss: 0.8011083602905273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "sigmoid_layer = nn.Sigmoid()\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "y_pred = torch.tensor([0.4498, 0.8845, 0.4576, 0.6450, 0.2304])\n",
        "y_true = torch.tensor([0.0, 1.0, 1.0, 0.0, 1.0])\n",
        "\n",
        "#sigmoid layer\n",
        "y_logit  = sigmoid_layer(y_pred)\n",
        "\n",
        "#loss\n",
        "output = F.binary_cross_entropy(y_true, y_logit)\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NiBl9A4zvWl",
        "outputId": "9f793b93-9914-49cc-f943-6bd9e959ed0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(47.7786)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "y_pred = torch.tensor([0.4498, 0.8845, 0.4576, 0.6450, 0.2304])\n",
        "y_true = torch.tensor([0.0, 1.0, 1.0, 0.0, 1.0])\n",
        "\n",
        "loss = F.binary_cross_entropy_with_logits(y_true, y_pred)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhGaSvl8z8Wl",
        "outputId": "64b61a3d-fa83-4560-9195-6b030f001a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7507)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "$$INCOMPLETE:$$ $$WORKING$$ $$ON$$ $$CODE$$ $$UNDERSTANDING$$\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "en_e5VWR3QyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{2. Categorical Cross Entropy Loss (CCE) Loss}$"
      ],
      "metadata": {
        "id": "_faXIR3I3r5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Use Case:** Multi-class Classification tasks  \n",
        "\n",
        "### **Formula:**  \n",
        "$$\\text{CCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$$\n",
        "\n",
        "$where:$  \n",
        "- $n$ = total number of observations  \n",
        "- $C$ = total number of classes  \n",
        "- $y_{i,c}$ = binary indicator ($0$ or $1$)$:$ $1$ if class $(c)$ is the true label for for observation $(i)$, else $0$\n",
        "- $\\hat{y}_{i,c}$ = predicted probability for class $(c)$ for observation $(i)$\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "---\n",
        "$\\textbf{Note: CCE has characteristics, advantage and disadvantage very similar to\n",
        "BCE}$\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Advantages**  \n",
        "\n",
        "---\n",
        "\n",
        "- **Directly Interpretable in Probability Space:** Outputs are probabilities for each class, making it straightforward to interpret and evaluate model performance for multi-class classification tasks.  \n",
        "**Logarithmic Penalization:** Assigns high penalty to incorrect predictions, especially if the predicted probability for the true class is low. The logarithmic function penalizes incorrect predictions more severely the more confident the incorrect prediction is.\n",
        "- **Sensitivity to Confidence:** Penalizes confidently incorrect predictions, encouraging the model to output accurate class probabilities.  \n",
        "- **Smooth and Differentiable:** Suitable for gradient-based optimization methods, as the loss function is smooth and differentiable.\n",
        "\n",
        "### **Disadvantages**  \n",
        "\n",
        "---\n",
        "- **Sensitive to Imbalanced Datasets:** If some classes are underrepresented, the loss can be dominated by the majority classes, potentially leading to poor performance on minority classes.  \n",
        "- **Overconfident Predictions:** Similar to BCE, large errors from very high or very low predicted probabilities can lead to unstable gradients during training.\n",
        "\n",
        "### **When to Use CCE**  \n",
        "\n",
        "---\n",
        "\n",
        "- When performing multi-class classification and you need the model to output probabilities for each class.  \n",
        "- Suitable when each class has equal significance or when the cost of misclassification is balanced across classes.\n"
      ],
      "metadata": {
        "id": "u1tlvyof4NdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Implementation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YjUisrFe8J8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "y_true = torch.tensor([2, 0, 1])\n",
        "y_pred = torch.tensor([\n",
        "    [0.1, 0.2, 0.7],\n",
        "    [0.8, 0.1, 0.1],\n",
        "    [0.2, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(y_pred, y_true)\n",
        "print(f'Categorical Cross Entropy Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CUtjjyj7Z-b",
        "outputId": "55bfd600-464f-4716-ed65-8d5ba4f53653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Cross Entropy Loss: 0.7418753504753113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    n_samples = y_true.shape[0]\n",
        "    cce_loss = -np.sum(y_true * np.log(y_pred)) / n_samples\n",
        "    return cce_loss\n",
        "\n",
        "y_true = np.array([\n",
        "    [0, 0, 1],\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0]\n",
        "])\n",
        "\n",
        "y_pred = np.array([\n",
        "    [0.1, 0.2, 0.7],\n",
        "    [0.8, 0.1, 0.1],\n",
        "    [0.2, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "loss = categorical_cross_entropy(y_true, y_pred)\n",
        "print(f'Categorical Cross Entropy Loss: {loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1TpmXXH83sz",
        "outputId": "18577e78-a619-430f-ff3a-06c98d29748b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Cross Entropy Loss: 0.3121644797305582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{3. BCE Vs. CCE}$\n",
        "\n",
        "\n",
        "| **Aspect**                   | **Binary Cross Entropy (BCE)**                              | **Categorical Cross Entropy (CCE)**                        |\n",
        "|------------------------------|--------------------------------------------------------------|-------------------------------------------------------------|\n",
        "| **Use Case**                 | Binary classification tasks with two classes                | Multi-class classification tasks with more than two classes |\n",
        "| **Formula**                  | $-\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$ | $-\\sum_{i=1}^{C} y_{i} \\log(\\hat{y}_{i})$       |\n",
        "| **Output Range**             | Probabilities between 0 and 1 (using sigmoid function)      | Probability distribution summing to 1 (using softmax function) |\n",
        "| **Logarithmic Penalization** | Penalizes high-confidence incorrect predictions heavily      | Penalizes incorrect predictions based on confidence for the true class |\n",
        "| **Activation Function**      | Sigmoid function                                              | Softmax function                                             |\n",
        "| **Key Characteristics**      | - Logarithmic penalization <br> - Direct output probability  | - Logarithmic penalization <br> - Output probability distribution |\n",
        "| **Training Context**         | Outputs a probability for a single class                     | Outputs probabilities for multiple classes                  |\n",
        "| **Usage**                    | Suitable for binary classification problems                  | Suitable for multi-class classification problems           |\n"
      ],
      "metadata": {
        "id": "REz4imCF63Ii"
      }
    }
  ]
}